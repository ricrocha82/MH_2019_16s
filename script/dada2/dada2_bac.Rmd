---
title: "dada2_pipeline"
author: "Ricardo Silva"
date: "5/20/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval=FALSE, 
                      message=FALSE, 
                      warning=FALSE)
```

```{r CLEAR EVERYTHING, eval=FALSE, include=FALSE}
# unload all non-base packages
lapply(names(sessionInfo()$otherPkgs), function(pkgs)
  detach(
    paste0('package:', pkgs),
    character.only = T,
    unload = T,
    force = T
))

rm(list=ls())
```

```{r load libraries, message=FALSE, warning=FALSE, include=FALSE}
suppressPackageStartupMessages({
  library(dada2)
  library(Biostrings) # for primer identification
  library(tidyverse)
  library(ShortRead) # for primer identification
  library(patchwork)
})
packageVersion("dada2")
```


```{r}
folder_path <- "~/R/git_hub/MH_2019/"
#setwd(folder_path)
source(paste0(folder_path,"/scripts/theme_publication.R"))
files_path <- paste0(folder_path,"/data/sequences_fasta_16S")
head(list.files(files_path))
```

These fastq files were generated by 2x300bp Illumina Miseq amplicon sequencing of the V1-3 region of the 16S rRNA gene from water column samples collected along a transect in a semi-enclosed marine environment located in the west coast of Tasmania, Austrialia. 
DADA2 expects there to be fastq file(s) for each samples (in our case one forward and one reverse, for each sample).
If the samples are demultiplexed without non-biological nucleotides (e.g. primers, adapters, linkers, etc.), dada2 pipeline proceeds as follows:

Filter and trim: `filterAndTrim()`
Dereplicate: `derepFastq()`
Learn error rates: `learnErrors()`
Infer sample composition: `dada()`
Merge paired reads: `mergePairs()`
Make sequence table: `makeSequenceTable()`
Remove chimeras: `removeBimeraDenovo()`

<https://benjjneb.github.io/dada2/tutorial.html>
<https://astrobiomike.github.io/amplicon/dada2_workflow_ex>
<https://bioconductor.org/packages/devel/bioc/vignettes/dada2/inst/doc/dada2-intro.html>

# Preparation

```{r names}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(files_path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(files_path, pattern="_R2_001.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- fnFs %>% basename() %>% str_split("_") %>% map_chr(., 1) 
# sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

#increasing R memory
options(future.globals.maxSize = 20000 * 1024^2)
```

# Identify primers

```{r primers}
# Bacterial 16S rRNA gene (27F-519R)
FWD <- "AGAGTTTGATCMTGGCTCAG" #  27F (Lane 1991) and 
REV <- "GWATTACCGCGGCKGCTG" # 519R (Lane et al. 1993)

```

```{r complements}
# function to generate all possible orientations based on primer sequence
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    #require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, 
                 Complement = complement(dna), 
                 Reverse = reverse(dna), 
                 RevComp = reverseComplement(dna))
    return(map_chr(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

```

Identifying and counting the primers on one set of paired end FASTQ files is sufficient, assuming all the files were created using the same library preparation, so we’ll just process the first sample.

```{r locating}
# function do detect primers in the sequences
primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

# table with the results
rbind(FWD.ForwardReads = map2(FWD.orients, fnFs[[6]], primerHits), # forward primer (in normal orientation)
    FWD.ReverseReads = map2(FWD.orients, fnRs[[6]], primerHits),  # reverse complement of the reverse primer
    REV.ForwardReads = map2(REV.orients, fnFs[[6]], primerHits),  # reverse complement of the reverse primer
    REV.ReverseReads = map2(REV.orients,  fnRs[[6]], primerHits)) # reverse primer (in normal orientation)

```

# Remove Primers

The FWD and REV primers were found in some forward and reverse reads in its reverse orientation, respectively. But, there appear to be a very small number (~200) of reads that still have some primers on them, but they won't interfere with subsequent analysis. <https://github.com/benjjneb/dada2/issues/675>

```{r cutadapt}
#cutadapt <- "/usr/local/lib/python3.8/dist-packages/cutadapt" # CHANGE ME to the cutadapt path on your machine
#system2(cutadapt, args = "--version") # Run shell commands from R
```

If the above command succesfully executed, R has found cutadapt and you are ready to continue following along.

```{r running cutadapt, eval= FALSE, include=FALSE}
#path.cut <- file.path(files_path, "cutadapt")
#if(!dir.exists(path.cut)) dir.create(path.cut)
#fnFs.cut <- file.path(path.cut, basename(fnFs))
#fnRs.cut <- file.path(path.cut, basename(fnRs))

#FWD.RC <- dada2:::rc(FWD)
#REV.RC <- dada2:::rc(REV)

# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
#R1.flags <- paste0("-a ^", FWD,"...", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
#R2.flags <- paste0("-A ^", REV, "...", FWD.RC) 

# Run Cutadapt
#for(i in seq_along(fnFs)) {
#  system2(cutadapt, args = c(R1.flags, R2.flags, "-m", 215,"-M",285, "--discard-untrimmed", # -n 2 required to remove FWD and REV from reads
#                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
#                             fnFs[i], fnRs[i])) # input files
#}
```

# Calucalting overlap bp and Where to trim

<https://forum.qiime2.org/t/where-to-trim-reads/5741/22>
<https://forum.qiime2.org/t/quality-score-and-trimming-in-dada2/541/2>
<https://github.com/benjjneb/dada2/issues/195>

Our paired-end reads cover the V1-V3 regions using the 27F (Lane 1991) and 519R (Lane et al. 1993) primers. It should yield an amplicon with 492 bp long. In calculating the length of overlapping bases we get 108 bp (600 - 492) overlap. If we truncate both forward and reverse at 290 and 240, respectively, the overlap would be 23 bp (492 - 290 - 225).

NOTE!! For the F/R reads to be successfully merged, `trunc-len-f` + `trunc-len-r` must be greater than the length of the amplicon + 20 nucleotides (the 20 nts is the length of the overlap). <https://github.com/qiime2/q2-dada2/issues/52>


# Inspect quality profles

Visualize the quality profiles of the Forward and reverse reads.

In gray-scale is a heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position.

```{r quality profiles}
# make a string to subset
n <- 1:length(fnFs)
samples.str <- paste0("-",n,"_") 

# map function F seqs
quality.plotsF <- split(n, ceiling(seq_along(n)/6)) %>% 
  map(. %>% 
        map(., ~str_subset(fnFs, samples.str[.])) %>% plotQualityProfile()
      ) %>% 
  map(~wrap_plots(.))
# save plots
map2(paste0(folder_path,"/output/dada2/bac/quality_control_F/group_sample_",names(quality.plotsF),".tiff"),quality.plotsF, ggsave)

# map function R seqs
quality.plotsR <- split(n, ceiling(seq_along(n)/6)) %>% 
  map(. %>% 
        map(., ~str_subset(fnRs, samples.str[.])) %>% plotQualityProfile()
      ) %>% 
  map(~wrap_plots(.))
# save plots
map2(paste0(folder_path,"/output/dada2/bac/quality_control_R/group_sample_",names(quality.plotsR),".tiff"),quality.plotsR, ggsave)

```

# Quality trimming/filtering

```{r filtering}

# Assign the filenames for the filtered fastq.gz files.
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(files_path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(files_path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

# run the function on good sequences
filtered_out <- filterAndTrim(fnFs, # “forward_reads” - input
                                   filtFs,  # “forward_reads” - output filtered
                                   fnRs, # “reverse_reads” - input
                                   filtRs,  # “forward_reads” - output filtered
                                   maxEE=c(2,2), # quality filtering threshold
                                   rm.phix=TRUE, # removes any reads that match the PhiX bacteriophage genome
                                #   minLen=30, # minimum length reads we want to keep after trimming
                                   truncLen=c(290,225), # minimum size to trim the forward and reverse reads (keep the QS above 30 overall) - Shorter sequences are discarded
                                   compress = TRUE, # gzipped fastq files 
                                   truncQ = 2, #  trims all bases after the first quality score of 2
                                   multithread=FALSE, # need to be set on Windows
                                   verbose = T) 

head(filtered_out)

# create new folder
path.dada2 <- file.path(files_path, "dada2")
if(!dir.exists(path.dada2)) dir.create(path.dada2)
rm(path.dada2)

# save
saveRDS(filtered_out, paste0(files_path, "/dada2/filtered_trim.RDS"))


```


```{r checking}
plotQualityProfile(filtFs[1:3])
plotQualityProfile(filtRs[1:3])
```

# Learn the Error Rates

We will generate a parametric error model of our data by learning the specific error-signature of our dataset. 

We can use `multithread=TRUE` on Windows (and probably should if computation time is an issue) for every command except `filterAndTrim`. <https://github.com/benjjneb/dada2/issues/1100>

```{r error rate}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

# save RDS
saveRDS(errF, paste0(files_path, "/dada2/errF.RDS"))
saveRDS(errF, paste0(files_path, "/dada2/errR.RDS"))
```

The plots below show the error rates for each possible transition (A→C, A→G, …) are shown. 
The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score.
We want the observed (black dots) to track well with the estimated (black line).
Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. Everything looks reasonable and we proceed with confidence.

```{r checking sanity}
# sanity check
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```

# Dereplication

The next thing we want to do is “dereplicate” the filtered fastq files. During dereplication, we condense the data by collapsing together all reads that encode the same sequence, which significantly reduces later computation times. Instead of keeping 100 identical sequences and doing all downstream processing to all 100, you can keep/process one of them, and just attach the number 100 to it.

```{r dereplication}
derep_forward <- derepFastq(filtFs, verbose=TRUE)
names(derep_forward) <- sample.names
derep_reverse <- derepFastq(filtRs, verbose=TRUE)
names(derep_reverse) <- sample.names

# save RDS
saveRDS(derep_forward, paste0(files_path, "/dada2/derep_forward.RDS"))
saveRDS(derep_reverse, paste0(files_path, "/dada2/derep_reverse.RDS"))

#derep_forward <- read_rds(paste0(files_path, "/dada2/derep_forward.RDS"))
#derep_reverse <- read_rds(paste0(files_path, "/dada2/derep_reverse.RDS"))
```


# Inferring ASVs/ Sample composition inference

Here’s where DADA2 gets to do what it was born to do, that is to do its best to infer true biological sequences. Pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. `dada(..., pool="pseudo")` performs pseudo-pooling, in which samples are processed independently after sharing information between samples, approximating pooled sample inference in linear time. In many cases, especially when samples are repeatedly drawn from the same source such as in longitudinal experiments, pseudo-pooling can provide a more accurate description of ASVs at very low frequencies (e.g. present in 1-5 reads per sample). 

```{r ASV inference}
dada_forward <- dada(derep_forward, err=errF, 
                   #  pool="pseudo", 
                     multithread=TRUE)
# save RDS
saveRDS(dada_forward, paste0(files_path, "/dada2/dada_forward.RDS"))
#dada_forward <- read_rds(paste0(files_path, "/dada2/dada_forward.RDS"))

dada_reverse <- dada(derep_reverse, err=errR, 
                     #pool="pseudo", 
                     multithread=TRUE)
# save RDS
saveRDS(dada_reverse, paste0(files_path, "/dada2/dada_reverse.RDS"))
#dada_reverse <- read_rds(paste0(files_path, "/dada2/dada_reverse.RDS"))
```

Inspecting the returned dada-class object:

```{r insp dada obj}
dada_forward[[6]]
dada_reverse[[6]]
```

# Merging paired forward and reverse reads

Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences.  By default it requires that at least 12 bps overlap. We are going to merge using three diffrent ways and see how they go:
1 - default (DADA2 default)
2 - trimOverhang = T; and
3 - minimum overlap (`minOverlap`) at 30 with the `trimOverhang` option to `TRUE` in case any of our reads go passed their opposite primers

The 1 function returns a `data.frame` corresponding to each successfully merged unique sequence. The `$forward` and `$reverse` columns record which forward and reverse sequence contributed to that merged sequence.

```{r merging}
merged_amplicons.default <- mergePairs(dada_forward, derep_forward, 
                               dada_reverse, derep_reverse, 
                             #  trimOverhang = T,
                             #  minOverlap=30, 
                               verbose = T)

merged_amplicons <- merged_amplicons.default

head(merged_amplicons[[4]])
# this object holds a lot of information that may be the first place you'd want to look if you want to start poking under the hood
class(merged_amplicons) # list
length(merged_amplicons) # 59 elements in this list, one for each of our samples
names(merged_amplicons) # the names() function gives us the name of each element of the list 

class(merged_amplicons$`RRPS-MH-1`) # each element of the list is a dataframe that can be accessed and manipulated like any ordinary dataframe

names(merged_amplicons$`RRPS-MH-1`) # the names() function on a dataframe gives you the column names
# "sequence"  "abundance" "forward"   "reverse"   "nmatch"    "nmismatch" "nindel"    "prefer"    "accept"


# save RDS
saveRDS(merged_amplicons.default, paste0(files_path, "/dada2/merged_amplicons.default.RDS"))

#merged_amplicons.default <- read_rds(paste0(files_path, "/dada2/merged_amplicons.default.RDS"))

# remove big objects
rm("derep_forward", 'derep_reverse')
```

# Construct sequence table

We can see from the dimensions of the “seqtab” matrix that we have 25,405 ASVs in this case. It seems the diffrence between default and trimOverhang = T is minimal. We will continue with the default merged data.

```{r seqtable}
seqtab.overhand.T <- makeSequenceTable(merged_amplicons.overhang.T)
seqtab.default <- makeSequenceTable(merged_amplicons.default)
class(seqtab) # matrix
dim(seqtab.overhand.T) # 59 69954 -> trimOverhang = T
dim(seqtab.default) # 59 69996 -> default
dim(seqtab.min.30) # 59 25405 -> minOverlap = 30, trimOverhang = T

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

seqtab <- seqtab.default

rm('merged_amplicons', 'merged_amplicons.min.30', 'merged_amplicons.overhang.false', 'merged_amplicons.overhang.T', 'seqtab.overhand.T', 'seqtab.default', 'seqtab.min.30')
```

# Chimera identification

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs.

```{r chimera}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim) #   44491 bimeras out of 69996 input sequences
dim(seqtab.nochim.min.30) #  13680 bimeras out of 25405 input sequences

# lost 44491 sequences, we don't know if they held a lot in terms of abundance, this is one quick way to look at that
sum(seqtab.nochim)/sum(seqtab)  # 92%

saveRDS(seqtab.nochim, paste0(files_path, "/dada2/seqtab.nochim.RDS"))
```

Here chimeras make up about 53% of the merged sequence variants, but when we account for the abundances of those variants we see they account for less than 8% of the merged sequence reads.

# Overview of counts throughout

As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r overview}
# set a little function
getN <- function(x) sum(getUniques(x))


# making a little table
summary_tab <- data.frame(row.names = sample.names, 
                          dada2_input = filtered_out[,1],
                          filtered = filtered_out[,2], 
                          dada_f = sapply(dada_forward, getN),
                          dada_r = sapply(dada_reverse, getN), 
                          merged = sapply(merged_amplicons, getN),
                          nonchim = rowSums(seqtab.nochim),
                          final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))

summary_tab
```

# Assign taxonomy